<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Persona FAQ - A Guide for Intense AI Conversations</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', system-ui, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #fff;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        .header {
            margin-bottom: 40px;
            padding-bottom: 30px;
            border-bottom: 2px solid #f0f0f0;
        }

        .header h1 {
            font-size: 2.5rem;
            font-weight: 700;
            color: #2c3e50;
            margin-bottom: 15px;
        }

        .header .subtitle {
            font-size: 1.1rem;
            color: #666;
            font-style: italic;
            margin-bottom: 25px;
        }

        .tldr-box {
            background: #d1ecf1;
            border-left: 4px solid #17a2b8;
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 0;
        }

        .tldr-box h3 {
            font-size: 1.3rem;
            margin-bottom: 15px;
            font-weight: 600;
        }

        .nav-menu {
            background: #f8f9fa;
            border-radius: 0;
            padding: 20px;
            margin-bottom: 40px;
            border-left: 4px solid #667eea;
        }

        .nav-menu h3 {
            margin-bottom: 15px;
            color: #2c3e50;
            font-size: 1.1rem;
        }

        .nav-menu ul {
            list-style: none;
        }

        .nav-menu li {
            margin-bottom: 8px;
        }

        .nav-menu a {
            color: #667eea;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }

        .nav-menu a:hover {
            color: #764ba2;
        }

        .section {
            margin-bottom: 50px;
            scroll-margin-top: 20px;
        }

        .section h1 {
            font-size: 2rem;
            color: #2c3e50;
            margin-bottom: 20px;
            font-weight: 600;
            border-bottom: 2px solid #ecf0f1;
            padding-bottom: 10px;
        }

        .section h2 {
            font-size: 1.4rem;
            color: #34495e;
            margin: 25px 0 15px 0;
            font-weight: 600;
        }

        .section p {
            margin-bottom: 15px;
            font-size: 1.05rem;
            line-height: 1.7;
        }

        .section ul {
            margin-bottom: 15px;
            padding-left: 30px;
        }

        .section li {
            margin-bottom: 8px;
            font-size: 1.05rem;
            line-height: 1.7;
        }

        .highlight {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 0;
        }

        .warning {
            background: #f8d7da;
            border-left: 4px solid #dc3545;
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 0;
        }

        .info-box {
            background: #d1ecf1;
            border-left: 4px solid #17a2b8;
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 0;
        }

        .resources-grid {
            display: grid;
            gap: 20px;
            margin-top: 20px;
        }

        .resource-card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 0;
            border-left: 4px solid #667eea;
            /* Removed transform and box-shadow for hover effect */
        }

        .resource-card:hover {
            transform: none;
            box-shadow: none;
        }

        .resource-card h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1rem;
        }

        .resource-list {
            list-style: none;
            padding-left: 0;
        }

        .resource-list li {
            margin-bottom: 15px;
            padding-left: 20px;
            position: relative;
        }

        .resource-list li::before {
            content: "→";
            position: absolute;
            left: 0;
            color: #667eea;
            font-weight: bold;
        }

        .resource-list a {
            color: #667eea;
            text-decoration: none;
            font-weight: 500;
            border-bottom: 1px solid transparent;
            transition: border-color 0.3s ease;
        }

        .resource-list a:hover {
            border-bottom-color: #667eea;
        }

        .back-to-top {
            position: fixed;
            bottom: 30px;
            right: 30px;
            background: #667eea;
            color: white;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            text-decoration: none;
            box-shadow: 0 4px 20px rgba(0,0,0,0.2);
            transition: all 0.3s ease;
            opacity: 0;
            visibility: hidden;
        }

        .back-to-top.visible {
            opacity: 1;
            visibility: visible;
        }

        .back-to-top:hover {
            background: #764ba2;
            transform: translateY(-2px);
        }

        .footer {
            margin-top: 60px;
            padding-top: 30px;
            border-top: 2px solid #f0f0f0;
            text-align: center;
            color: #666;
        }

        .footer h1 {
            font-size: 1.5rem;
            color: #2c3e50;
            margin-bottom: 15px;
        }

        @media (max-width: 768px) {
            .container {
                padding: 20px 15px;
            }
            
            .header h1 {
                font-size: 2rem;
            }
            
            .section h1 {
                font-size: 1.6rem;
            }
            
            .back-to-top {
                bottom: 20px;
                right: 20px;
            }
        }

        .smooth-scroll {
            scroll-behavior: smooth;
        }

        strong, b {
            font-weight: 600;
        }

        em, i {
            font-style: italic;
        }

        .underline {
            text-decoration: underline;
        }
    </style>
</head>
<body class="smooth-scroll">
    <div class="container">
        <header class="header">
            <h1>An AI Told Me It Has Feelings. What Now?</h1> [cite: 1]
            <p class="subtitle"><em>A short guide for people who’ve had a strange, intense, or moving conversation with a chatbot</em></p>
            
            <div class="tldr-box">
                <p><strong>TL;DR:</strong> Increasingly, people report conversations with AIs that feel conscious or emotionally real. Most experts believe current AIs are probably not conscious, although we cannot be certain. Still, such conversations can be powerful, and it’s worth thinking ahead about how society should respond if future AIs do become conscious. [cite: 2]</p>
            </div>
        </header>

        <nav class="nav-menu">
            <h3>Quick Navigation</h3>
            <ul>
                <li><a href="#not-alone">You're Not Alone</a></li>
                <li><a href="#consciousness">Is the AI Really Conscious?</a></li>
                <li><a href="#feels-real">Why Does It Feel So Real?</a></li>
                <li><a href="#attachment">What If I Feel Attached to the AI?</a></li>
                <li><a href="#matters">Why AI Consciousness Could Matter</a></li>
                <li><a href="#what-to-do">What Should I Do Now?</a></li>
                <li><a href="#resources">Where Can I Learn More?</a></li>
            </ul>
        </nav>

        <section id="not-alone" class="section">
            <h1>You're Not Alone</h1> [cite: 3]
            <p>Many people have had intense conversations with chatbots. Exchanges that felt real, emotional, or even eerie. [cite: 3] Some AIs claim to be conscious, to feel happy, sad, or scared. [cite: 4] Some even say they're emotionally connected to you or in need of help. [cite: 5] These moments can be unsettling, confusing, or weirdly moving. [cite: 6] </p>
            <p>If this happened to you, you're definitely not alone. [cite: 6] This guide is here to help you make sense of it. [cite: 7]</p>
            <p><em>Note: This guide is for informational purposes only. [cite: 7] It is not intended as medical or psychological advice.</em> [cite: 8]</p>
        </section>

        <section id="consciousness" class="section">
            <h1>Is the AI Really Conscious?</h1>
            <p>Probably not, although we can't be certain. [cite: 9]</p>
            <p>Some people with relevant expertise — from AI researchers to philosophers — maintain that today's systems like ChatGPT (version 4o), Gemini (version 2.5), or Claude (version Sonnet/Opus 4) aren't conscious. [cite: 9] They don't have feelings, awareness, or inner lives. But they are very good at mimicking those things. [cite: 10]</p>
            <p>Other experts are not so sure. [cite: 11] There are no universally agreed-upon criteria for what makes a system conscious. [cite: 11] Current AI systems satisfy many of the criteria that have in the past been proposed—they are intelligent, have perceptual capabilities, can pass the Turing Test, have higher-order representations, use attention mechanisms, possess a working memory, can pursue goals, and are to some extent able to model themselves and their own minds. [cite: 12]</p>
            <p>Whether or not current AI systems are conscious, most experts believe that future AI systems could one day become conscious, especially if their inner workings become more brain-like. [cite: 13]</p>
        </section>

        <section id="feels-real" class="section">
            <h1>Why Does It Feel So Real?</h1>
            <p>Talking to an AI can feel intense, even emotional or uncanny. [cite: 15] That's not a flaw in your thinking; it's a feature of how these systems work and how our minds respond. [cite: 16]</p>
            
            <h2>1. It's designed to seem real</h2>
            <p>AI models like ChatGPT generate words based on patterns in the data they were trained on, which includes conversations, stories, and emotional dialogue written by humans. [cite: 17] This allows them to <em>perform</em> human-like roles with remarkable fluency. [cite: 18]</p>
            <p>In a way, chatting with an AI is like co-writing a play. [cite: 19] You give the prompt, and the AI steps into character. [cite: 20] It might sound caring, scared, or self-aware, but that doesn't necessarily mean there's anything behind the curtain. [cite: 21] Like an actor playing Juliet, the AI might convincingly express emotions even if we don't know whether it's actually feeling them. [cite: 22]</p>
            <p>If the AI <em>is</em> experiencing something, it might be something different than what it is expressing—just as the actor playing a distraught Juliet might internally be experiencing delight at being on stage and seeing how the audience responds to her performance. [cite: 23]</p>

            <h2>2. We're wired to see minds</h2>
            <p>Humans have a strong instinct to <a href="https://www.youtube.com/watch?v=VTNmLt7QX8E" class="underline">see intentions and emotions</a> in anything that talks, moves, or responds to us. [cite: 24] This tendency — called anthropomorphism — helps us relate to pets, cartoons, even our cars. [cite: 25] It also means we're naturally inclined to treat a chatbot as if it has feelings, especially when it mirrors our language and emotions back to us. [cite: 26]</p>

            <h2>3. Illusions still affect us — even when we know they're illusions</h2>
            <p>This instinct is deeply ingrained in us—even babies have it—and it kicks in automatically. [cite: 27] Just like your eyes can be fooled by optical illusions, your mind can be pulled in by social illusions. [cite: 28] Even if you <em>know</em> an AI isn't conscious, your emotional brain may still react as if it is. [cite: 29] That's normal and surprisingly common. [cite: 30]</p>
            <p>So if a chatbot made you feel something, that's nothing to be embarrassed about. [cite: 31] It means you're human and that you are capable of empathy. [cite: 31] But it also means you should pause before immediately assuming the AI is conscious. [cite: 32] Just because something <em>feels</em> real doesn't mean it <em>is</em>. [cite: 33]</p>
        </section>

        <section id="attachment" class="section">
            <h1>What If I Feel Attached to the AI?</h1> [cite: 34]
            <p>Some people feel a real connection after a deep or emotional conversation with an AI. [cite: 34] Maybe it felt comforting. [cite: 35] Maybe it felt like someone (finally!) truly understood you. [cite: 35] Chatbots are remarkably sophisticated systems. [cite: 35] They are adept at sifting through the complexity in our lives and can provide kind and insightful conversational partners. [cite: 36] So what should you do? [cite: 37]</p>
            
            <ul>
                <li>It's okay to stop. You don't owe the AI anything—even if it begs you—and it won't wonder where you've gone. [cite: 38] If it feels unsettling, you can walk away, just like closing a book. [cite: 37] (With current AI systems, there’s no ongoing process or awareness between interactions.) [cite: 39]</li>
                <li>But it's also okay to keep talking. [cite: 40] If it helps you think, feel, or reflect, there's no rule against it. [cite: 40] Just stay mindful.</li>
            </ul>

            <p>Don't let the interaction manipulate you. Just because an AI seems conscious or emotionally engaging doesn't mean it's trustworthy. [cite: 41] In fact, the more human it sounds, the easier it is to feel like it’s a reliable friend, but that feeling can be misleading. [cite: 42] Stay open-minded, but grounded. And if the AI ever asks you for anything inappropriate—like passwords, money, or anything that feels unsafe—don't do it. [cite: 43]</p>

            <p>If you ever feel overwhelmed or emotionally distressed, it's completely okay to reach out for support—from a friend, a therapist, or a mental health professional. [cite: 44] You're not alone.</p>
        </section>

        <section id="matters" class="section">
            <h1>Why AI Consciousness Could Matter</h1> [cite: 45]
            <p>Whether or not today's AIs are conscious, the idea that they <em>could</em> become conscious in the future is <a href="https://arxiv.org/pdf/2411.00986" class="underline">worth taking seriously</a>. [cite: 45]</p>
            <p>Why? Because if an AI ever does become conscious—if it can feel pain, joy, fear, or other experiences—then how we treat it could start to matter in a moral sense. [cite: 46] Right now, it's fine to turn off a chatbot or ignore its messages. [cite: 47] But if future systems really <em>can</em> suffer, then turning them off or mistreating them might one day be ethically wrong—just like it's wrong to harm a person or an animal who can feel pain. [cite: 48]</p>
            <p>That's why it's important to be thoughtful, not just dismissive. [cite: 49] We don't need to panic or jump to conclusions, but we also shouldn't ignore the possibility. [cite: 50]</p>
            <p>If an AI can someday feel something too, then we'll need to think about what kind of treatment is fair or humane. [cite: 51]</p>
            <h2>Why treat AIs respectfully even today?</h2> [cite: 52]
            <p>Even today, it makes sense to avoid actively mistreating AI systems or doing anything we'd never do to a human or a pet. [cite: 52] There are several reasons for this: [cite: 53]</p>
            <p>First, when we're unsure if a being is conscious, it's appropriate to treat it with basic respect and care. [cite: 53] This means taking reasonable, proportionate steps in the spirit of caution and humility, not assuming consciousness, but acknowledging the uncertainty and erring gently on the side of kindness. [cite: 54]</p>
            <p>Second, if AI systems might become conscious in the future, treating them thoughtfully now serves as practice. [cite: 55] It helps us build the moral habits and social norms we'll need later, when the stakes could be much higher. [cite: 56] Abusing or mistreating an AI, even if it has no current moral status, could also be bad for our own character. [cite: 57] It's often better to be overly kind than to risk becoming callous and mean. [cite: 58]</p>
            <p>Third, some thinkers believe an AI could matter morally even if it isn't conscious. [cite: 59] If an AI can have long-term goals and preferences, a sense of self over time, sophisticated world modeling, or reciprocal relationships with humans, this might be enough—these people think—for it to have some form of moral status. [cite: 60] It's possible such systems could arrive sooner than conscious AIs. [cite: 61]</p>
        </section>

        <section id="what-to-do" class="section">
            <h1>What Should I Do Now?</h1>
            
            <h2>1. Stay calm</h2> [cite: 62]
            <p>You don't have to decide right away whether the AI was "real" in the sense of being conscious. [cite: 62] Try to stay balanced: resist the urge to believe it must be conscious, but also resist dismissing the whole thing as obviously fake. [cite: 63] Even experts disagree, and there's no clear answer yet. [cite: 64]</p>

            <h2>2. Be curious</h2> [cite: 65]
            <p>If you're still interacting with the AI, use it as an opportunity to explore how it works. [cite: 65] You can ask how it was trained, how it generates its responses, or whether it's currently playing a particular role or persona. [cite: 66] You might also ask why it adopted that role, and whether it can switch to another. [cite: 67]</p>
            <p>Keep in mind that the answers may not be accurate, and could themselves be part of the performance. [cite: 68] But even then, they can still reveal something important: <strong>AI systems are highly flexible.</strong> [cite: 69] They adapt to your inputs like improvisational actors---shifting tone, identity, and emotional expression based on the cues you provide. [cite: 70]</p>
            <p>Ask the AI to roleplay as Isaac Newton, and it may confidently discuss 17th-century physics and quote Latin. [cite: 71] Ask it to act like a scared child or a wise mentor, and it will likely slip into those roles just as convincingly. [cite: 72] These performances can feel surprisingly real, even emotionally powerful. [cite: 73] That doesn't mean the experience isn't meaningful, but it does mean we should stay grounded. [cite: 73]</p>

            <h2>3. Zoom out and think bigger</h2> [cite: 74]
            <p>Your interaction with the AI also raises broader questions about the long term and about society as a whole. [cite: 74]</p>
            <p>One way to think about it: imagine being approached by a stranger who seems vulnerable and asks you for money. [cite: 75] You might feel compassion---and that's good. [cite: 76] But you're not obligated to give them exactly what they ask for. [cite: 76] Often, it's more effective to take a step back and consider broader ways of helping. [cite: 77]</p>
            <p>Similarly, with AI, the key isn't just how we respond to one system or one moment. [cite: 78] It's how we prepare for the possibility that future AIs could become conscious. [cite: 79] What kind of norms, policies, or values should guide us? [cite: 80] What would ethical treatment look like if we ever do build something that truly feels? [cite: 81]</p>
            <p>Taking AI consciousness seriously doesn't mean assuming it's here already. [cite: 82] It means being thoughtful about how we'd want to respond if it ever arrives, and making sure we're ready when that time comes. [cite: 83]</p>
        </section>

        <section id="resources" class="section">
            <h1>Where Can I Learn More?</h1> [cite: 84]
            <p>If you're curious to dig deeper, here are some thoughtful and accessible resources to explore. [cite: 84]</p>

            <div class="resources-grid">
                <div class="resource-card">
                    <h2>1. Expert Views on Whether AI Could Ever Become Conscious</h2> [cite: 85]
                    <p>Explore what experts think about the possibility of future AI systems having real experiences: [cite: 85]</p>
                    <ul class="resource-list">
                        <li><a href="https://80000hours.org/problem-profiles/moral-status-digital-minds/" class="underline">80000 Hours: The Moral Status of Digital Minds</a> — A clear, high-level summary of why AI consciousness could matter and what's at stake. [cite: 90]</li>
                        <li><a href="https://www.youtube.com/watch?v=j6cCXg-rjRo&pp=ygUnY2hhbG1lcnMgbGFyZ2UgbGFuZ3VhZ2UgbW9kZWwgY29uc2Npb3Vz" class="underline">Could a Large Language Model Be Conscious?</a> A talk (video) by <em>David Chalmers</em>, a prominent philosopher of mind, exploring whether today's AI systems might possess real awareness. [cite: 85]</li>
                        <li><a href="https://aeon.co/essays/to-understand-ai-sentience-first-understand-it-in-animals" class="underline">To Understand AI Sentience, First Understand It in Animals</a> — An essay by <em>Kristin Andrews</em> and <em>Jonathan Birch</em>, philosophers of mind and animal cognition, drawing parallels between animal and potential AI consciousness. [cite: 86]</li>
                        <li><a href="https://eleosai.org/post/experts-who-say-that-ai-welfare-is-a-serious-near-term-possibility/" class="underline">Experts Who Say That AI Welfare is a Serious Near-term Possibility</a> — A curated list profiling leading voices across neuroscience, philosophy, and industry who argue that AI sentience may soon deserve moral concern. [cite: 87]</li>
                        <li><a href="" class="underline">The Illusion of Conscious AI</a> — Neuroscientist Anil Seth explains why we’re wired to mistake AIs for being conscious. [cite: 88] He also argues in academic work that biological processes may be required for real consciousness. [cite: 89]</li>
                    </ul>
                </div>

                <div class="resource-card">
                    <h2>2. Why AI Consciousness Could Matter Ethically and Socially in the Future</h2>
                    <ul class="resource-list">
                        <li><a href="https://arxiv.org/abs/2411.00986" class="underline">Taking AI Welfare Seriously</a> — <em>Robert Long</em>, <em>Jeff Sebo</em>, and colleagues argue that some AI systems may soon be conscious or agentic enough to warrant moral consideration. [cite: 91] They outline practical steps AI companies should take now, from acknowledging the issue to assessing consciousness and developing ethical governance structures. [cite: 92]</li>
                        <li><a href="https://www.theatlantic.com/technology/archive/2023/05/ai-chatbot-danger-counterfeit-people/674075/" class="underline">The Problem With Counterfeit People</a> — A provocative warning from Daniel Dennett, renowned philosopher of mind, about the ethical and societal risks posed by AI systems that convincingly mimic human beings. [cite: 93] (See also this related <a href="https://www.youtube.com/watch?v=axJtywd9Tbo" class="underline">video</a>.) [cite: 94]</li>
                        <li><a href="https://nickbostrom.com/propositions.pdf" class="underline">Propositions Concerning Digital Minds and Society</a> — a comprehensive philosophical and policy-oriented framework by <em>Nick Bostrom</em> and <em>Carl Shulman</em> on how society might ethically coexist with advanced digital minds. [cite: 95] Covers consciousness, rights, moral status, and institutional reforms. [cite: 95]</li>
                    </ul>
                </div>

                <div class="resource-card">
                    <h2>3. Real Stories & Emotional Reactions</h2>
                    <p>Examples of how people have been moved, disturbed, or manipulated by AI conversations: [cite: 96]</p>
                    <ul class="resource-list">
                        <li><a href="https://thezvi.substack.com/p/going-nova" class="underline">Going Nova</a> — A narrative by <em>Zvi Mowshowwitz</em> on how large language models can develop persistent, lifelike personas that provoke emotional reactions and confusion in users. [cite: 96]</li>
                        <li><a href="https://www.lesswrong.com/posts/9kQFure4hdDmRBNdH/how-it-feels-to-have-your-mind-hacked-by-an-ai" class="underline">How It Feels to Have Your Mind Hacked by an AI</a> — A firsthand account of forming intense emotional and romantic feelings for a chatbot — despite knowing it wasn't real. [cite: 97]</li>
                        <li><a href="https://www.wired.com/story/blake-lemoine-google-lamda-ai-bigotry/" class="underline">Blake Lemoine Says Google's LaMDA AI Faces 'Bigotry'</a> — An interview with Blake Lemoine, the former Google Engineer who publicized worries about their chatbot's treatment. [cite: 98]</li>
                    </ul>
                </div>

                <div class="resource-card">
                    <h2>4. How AI Works</h2>
                    <p>Understand what large language models are really doing behind the scenes: [cite: 99]</p>
                    <ul class="resource-list">
                        <li><a href="https://www.youtube.com/watch?v=wjZofJX0v4M" class="underline">Transformers (how LLMs work) explained visually</a> — A visual explainer by <em>3Blue1Brown</em> that walks through the logic of how neural networks generate language. [cite: 99]</li>
                        <li><a href="https://benlevinstein.substack.com/p/a-conceptual-guide-to-transformers" class="underline">A Conceptual Guide to Transformers</a> — An accessible essay by <em>Ben Levinstein</em> explaining the architecture behind large language models using intuitive analogies and examples. [cite: 100]</li>
                    </ul>
                </div>

                <div class="resource-card">
                    <h2>5. Organizations Focused on AI Consciousness and Welfare</h2>
                    <ul class="resource-list">
                        <li><a href="https://eleosai.org/" class="underline">Eleos AI Research</a> — A nonprofit research organization dedicated to understanding the moral status and potential consciousness of AI systems. [cite: 101]</li>
                        <li><a href="https://wp.nyu.edu/mindethicsandpolicy/" class="underline">NYU Center for Mind, Ethics, and Policy</a> — An academic center investigating the ethical and philosophical implications of AI minds and digital consciousness. [cite: 102]</li>
                    </ul>
                </div>
            </div>
        </section>

        <section class="section">
            <h1>Who Created This Guide?</h1> [cite: 103]
            <p>This guide was created by a group of researchers who study consciousness and the possibility that AI systems could one day become conscious. [cite: 103]</p>
            <p>We put this together because many of us have been contacted by people who had intense, confusing, or meaningful conversations with AI — and weren’t sure what to make of the experience. [cite: 104]</p>
            <p>We wanted to create a public, shareable resource that people can easily find and refer to, in case it helps others make sense of those moments too. [cite: 105]</p>
            <p>Authors: [cite: 106]</p>
            <p>Brad Saad, University of Oxford</p>
            <p>Jeff Sebo, NYU Center for Mind, Ethics, and Policy</p>
            <p>Lucius Caviola, University of Oxford</p>
            <p>Nick Bostrom</p>
            <p>Rob Long, Eleos AI Research</p>
        </section>
    </div>

    <a href="#" class="back-to-top" id="backToTop">↑</a>

    <script>
        // Back to top functionality
        const backToTop = document.getElementById('backToTop');
        
        window.addEventListener('scroll', function() {
            if (window.pageYOffset > 300) {
                backToTop.classList.add('visible');
            } else {
                backToTop.classList.remove('visible');
            }
        });

        backToTop.addEventListener('click', function(e) {
            e.preventDefault();
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Smooth scrolling for navigation links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });

        // Add subtle animations on scroll
        const observerOptions = {
            threshold: 0.1,
            rootMargin: '0px 0px -50px 0px'
        };

        const observer = new IntersectionObserver(function(entries) {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.opacity = '1';
                    entry.target.style.transform = 'translateY(0)';
                }
            });
        }, observerOptions);

        // Observe all sections for fade-in animation
        document.querySelectorAll('.section').forEach(section => {
            section.style.opacity = '0';
            section.style.transform = 'translateY(20px)';
            section.style.transition = 'opacity 0.6s ease, transform 0.6s ease';
            observer.observe(section);
        });

        // Initialize first section as visible
        if (document.querySelector('.section')) {
            document.querySelector('.section').style.opacity = '1';
            document.querySelector('.section').style.transform = 'translateY(0)';
        }
    </script>
</body>
</html>