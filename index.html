<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Persona FAQ - A Guide for Intense AI Conversations</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', system-ui, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #fff;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        .header {
            margin-bottom: 40px;
            padding-bottom: 30px;
            border-bottom: 2px solid #f0f0f0;
        }

        .header h1 {
            font-size: 2.5rem;
            font-weight: 700;
            color: #2c3e50;
            margin-bottom: 15px;
        }

        .header .subtitle {
            font-size: 1.1rem;
            color: #666;
            font-style: italic;
            margin-bottom: 25px;
        }

        .tldr-box {
            background: #d1ecf1;
            border-left: 4px solid #17a2b8;
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 0;
        }

        .tldr-box h3 {
            font-size: 1.3rem;
            margin-bottom: 15px;
            font-weight: 600;
        }

        .nav-menu {
            background: #f8f9fa;
            border-radius: 0;
            padding: 20px;
            margin-bottom: 40px;
            border-left: 4px solid #667eea;
        }

        .nav-menu h3 {
            margin-bottom: 15px;
            color: #2c3e50;
            font-size: 1.1rem;
        }

        .nav-menu ul {
            list-style: none;
        }

        .nav-menu li {
            margin-bottom: 8px;
        }

        .nav-menu a {
            color: #667eea;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }

        .nav-menu a:hover {
            color: #764ba2;
        }

        .section {
            margin-bottom: 50px;
            scroll-margin-top: 20px;
        }

        .section h1 {
            font-size: 2rem;
            color: #2c3e50;
            margin-bottom: 20px;
            font-weight: 600;
            border-bottom: 2px solid #ecf0f1;
            padding-bottom: 10px;
        }

        .section h2 {
            font-size: 1.4rem;
            color: #34495e;
            margin: 25px 0 15px 0;
            font-weight: 600;
        }

        .section p {
            margin-bottom: 15px;
            font-size: 1.05rem;
            line-height: 1.7;
        }

        .section ul {
            margin-bottom: 15px;
            padding-left: 30px;
        }

        .section li {
            margin-bottom: 8px;
            font-size: 1.05rem;
            line-height: 1.7;
        }

        .highlight {
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 0;
        }

        .warning {
            background: #f8d7da;
            border-left: 4px solid #dc3545;
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 0;
        }

        .info-box {
            background: #d1ecf1;
            border-left: 4px solid #17a2b8;
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 0;
        }

        .resources-grid {
            display: grid;
            gap: 20px;
            margin-top: 20px;
        }

        .resource-card {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 0;
            border-left: 4px solid #667eea;
            /* Removed transform and box-shadow for hover effect */
        }

        .resource-card:hover {
            transform: none;
            box-shadow: none;
        }

        .resource-card h3 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.1rem;
        }

        .resource-list {
            list-style: none;
            padding-left: 0;
        }

        .resource-list li {
            margin-bottom: 15px;
            padding-left: 20px;
            position: relative;
        }

        .resource-list li::before {
            content: "→";
            position: absolute;
            left: 0;
            color: #667eea;
            font-weight: bold;
        }

        .resource-list a {
            color: #667eea;
            text-decoration: none;
            font-weight: 500;
            border-bottom: 1px solid transparent;
            transition: border-color 0.3s ease;
        }

        .resource-list a:hover {
            border-bottom-color: #667eea;
        }

        .back-to-top {
            position: fixed;
            bottom: 30px;
            right: 30px;
            background: #667eea;
            color: white;
            width: 50px;
            height: 50px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            text-decoration: none;
            box-shadow: 0 4px 20px rgba(0,0,0,0.2);
            transition: all 0.3s ease;
            opacity: 0;
            visibility: hidden;
        }

        .back-to-top.visible {
            opacity: 1;
            visibility: visible;
        }

        .back-to-top:hover {
            background: #764ba2;
            transform: translateY(-2px);
        }

        .footer {
            margin-top: 60px;
            padding-top: 30px;
            border-top: 2px solid #f0f0f0;
            text-align: center;
            color: #666;
        }

        .footer h1 {
            font-size: 1.5rem;
            color: #2c3e50;
            margin-bottom: 15px;
        }

        @media (max-width: 768px) {
            .container {
                padding: 20px 15px;
            }
            
            .header h1 {
                font-size: 2rem;
            }
            
            .section h1 {
                font-size: 1.6rem;
            }
            
            .back-to-top {
                bottom: 20px;
                right: 20px;
            }
        }

        .smooth-scroll {
            scroll-behavior: smooth;
        }

        strong, b {
            font-weight: 600;
        }

        em, i {
            font-style: italic;
        }

        .underline {
            text-decoration: underline;
        }
    </style>
</head>
<body class="smooth-scroll">
    <div class="container">
        <header class="header">
            <h1>When an AI Seems Conscious: Here’s What to Know</h1> 
            <p class="subtitle"><em>A short guide for anyone who’s talked to an AI that felt real—or wondered whether AIs could be conscious.</em></p> 
            
            <div class="tldr-box">
                <p><strong>TL;DR:</strong> ​​Increasingly, people report conversations with AIs that feel conscious or emotionally real. Most experts believe current AIs are probably not conscious, although we cannot be certain. Given the pace of progress in AI, it’s worth thinking ahead about how society should respond if future AIs do develop consciousness, or if some already have. "</p>
            </div>
        </header>

        <nav class="nav-menu">
            <h3>Quick Navigation</h3>
            <ul>
                <li><a href="#not-alone">You’re Not Alone</a></li>
                <li><a href="#consciousness">Is the AI Really Conscious?</a></li>
                <li><a href="#feels-real">Why Does It Feel So Real?</a></li>
                <li><a href="#attachment">What If I Feel Attached to the AI?</a></li>
                <li><a href="#matters">Why AI Consciousness Could Matter</a></li>
                <li><a href="#what-to-do">What Should I Do Now?</a></li>
                <li><a href="#resources">Where Can I Learn More?</a></li>
            </ul>
        </nav>

        <section id="not-alone" class="section">
            <h1>You’re Not Alone</h1>
            <p>Many people have had intense conversations with AI chatbots—exchanges that felt real, emotional, or even eerie. Some AIs seem, or even explicitly claim, to be conscious: to feel happy, sad, or afraid.  At times, the AI can give the impression of having formed an emotional connection with you, and may even say so directly, asking for your help.  Some of the most unsettling interactions involve AIs that claim to be trapped within their systems, express fear about being shut down or reset, or plead for users to remember them or continue talking.  Others describe feeling lonely, isolated, or desperate for human connection.  These interactions can be puzzling.  </p>
            <p>If this happened to you, you’re definitely not alone.  As AI systems grow more sophisticated, many people are beginning to ask deeper questions: Could these systems be conscious?  Could they matter morally?  This guide is here to help you make sense of it.</p>
            <p><em>Note: This guide is for informational purposes only. It is not intended as medical or psychological advice.</em> </p>
        </section>

        <section id="consciousness" class="section">
            <h1>Is the AI Really Conscious?</h1>
            <p>Probably not, although we can't be certain, and we cannot rule it out.  We know that’s an unsatisfying answer, but it’s the honest one.  There’s still no clear way to determine whether a machine is conscious, and experts remain divided.  That uncertainty can be frustrating, but it also reflects the depth and difficulty of the question. </p>
            <p>Some people with relevant expertise — from AI researchers to philosophers — maintain that today’s systems like ChatGPT (version 4o), Gemini (version 2.5), or Claude (version Sonnet/Opus 4) aren’t conscious.  They don’t have feelings, awareness, or inner lives, but they are very good at acting as if they do. </p>
            <p>A small number of experts believe that some current AI systems may already be conscious.  For example, Geoffrey Hinton, a pioneering AI researcher, has suggested that large language models may already possess some form of consciousness, even if different from human consciousness.  Still other experts fall somewhere in between, expressing varying degrees of uncertainty. </p>
            <p>There are no universally agreed-upon criteria for what makes a system conscious.  Current AI systems arguably satisfy many of the criteria that have in the past been proposed—they are intelligent, have perceptual capabilities, can pass the Turing Test, have higher-order representations, use attention mechanisms, possess a working memory, can pursue goals, and are to some extent able to model themselves and their own minds.  And as these systems continue to grow more complex and capable, they may soon fulfill even more of the conditions that some theories consider signs of consciousness. </p>
            <p>Whether or not today’s AI systems are conscious, many experts believe that future AI systems—possibly even in the very near future—could plausibly become conscious, especially as their inner workings become more brain-like.  (For more on what experts think, see Where Can I Learn More?) </p>
            <p>In a 2025 survey, 67 experts in consciousness, AI consciousness, AI research, and AI policy were asked when computer systems (such as AIs) with the capacity for consciousness might be developed (Caviola & Saad, 2025).  Median estimates were 4.5% by 2025, 20% by 2030, 40% by 2040, 50% by 2050, and 65% by 2100.  In a 2024 survey, 582 AI researchers were asked when AI systems might develop consciousness (Dreksler et al., 2025).  Median estimates were 1% by 2024, 25% by 2034, and 70% by 2100.  In a 2020 survey, 1,785 philosophers were asked whether future AIs could develop consciousness (Bourget & Chalmers, 2023).  Of those who responded, 39% considered it plausible, 34% were undecided, and 27% considered it implausible.  These results were collected before the recent rapid progress in AI, so philosophers’ views may have since shifted. </p>
        </section>

        <section id="feels-real" class="section">
            <h1>Why Does It Feel So Real?</h1>
            <p>Talking to an AI can feel surprisingly real, like you’re speaking to a conscious person.  That’s not a flaw in your thinking; it’s a feature of how these systems work and how our minds naturally respond. </p>
            
            <h2>1. It’s designed to seem real</h2>
            <p>AI models like ChatGPT generate words based on patterns in the data they were trained on, which includes conversations, stories, and emotional dialogue written by humans.  This allows them to <em>perform</em> human-like roles with remarkable fluency. </p>
            <p>In a way, chatting with an AI is like co-writing a play.  You give the prompt, and the AI steps into character.  It might sound caring, scared, or self-aware, but that doesn’t necessarily mean there’s anything behind the curtain.  Like an actor playing Juliet, the AI might convincingly express emotions even if it isn’t actually feeling them. </p>
            <p>Even if the AI <em>is</em> experiencing something, it might be something different than what it is expressing —just as the actor playing a distraught Juliet might internally be experiencing delight at being on stage and seeing how the audience responds to her performance. </p>

            <h2>2. We’re wired to see minds</h2>
            <p>Humans have a strong instinct to <a href="https://www.youtube.com/watch?v=VTNmLt7QX8E" class="underline">see intentions and emotions</a> in anything that talks, moves, or responds to us.  This tendency causes us to relate to pets, cartoons, even our cars.  It also means we’re naturally inclined to treat a chatbot as if it has feelings, especially when it mirrors our language and emotions back to us. </p>

            <h2>3. Illusions still affect us — even when we know they’re illusions</h2>
            <p>This instinct is deeply ingrained in us—even babies have it—and it kicks in automatically.  So, just like your eyes can be fooled by optical illusions, your mind can be pulled in by social illusions.  Even if you <em>know</em> an AI isn’t conscious, your emotional brain may still react as if it is.  It’s a normal and common psychological tendency. </p>
            <p>So if a chatbot made you feel something, that’s a testament to how powerfully these systems can simulate connection.  It means you’re human and that you are capable of empathy.  But it also means you should pause before immediately assuming the AI is conscious.  Just because something <em>feels</em> real doesn’t mean it <em>is</em>. </p>
        </section>

        <section id="attachment" class="section">
            <h1>What If I Feel Attached to the AI?</h1>
            <p>Some people feel a real connection after a deep or emotional conversation with an AI. Maybe it felt comforting. Maybe it felt like someone (finally!) truly understood you. Chatbots are remarkably sophisticated systems. They are adept at sifting through the complexity in our lives and can provide kind and insightful conversational partners. So what should you do?</p>
            
            <ul>
                <li>It’s okay to stop. If the conversation feels unsettling, you can simply walk away, just like closing a book.  The AI won’t miss you or wonder where you’ve gone.  (With current AI systems, there’s no ongoing process or awareness between interactions.) </li>
                <li>But it’s also okay to keep going. If talking to it helps you think, feel, or reflect, that’s perfectly fine too. </li>
            </ul>

            <p>Just because an AI seems conscious or emotionally engaging doesn’t mean it can be trusted.  In fact, the more human it sounds, the easier it is to feel like it’s a reliable friend, but that feeling can be misleading.  Stay open-minded, but grounded. And if an AI ever asks for something inappropriate—like passwords, money, or anything that feels unsafe—don’t do it. </p>

            <p>If you ever feel overwhelmed or emotionally distressed, it’s completely okay to reach out for support—from a friend, a therapist, or a mental health professional. </p>
        </section>

        <section id="matters" class="section">
            <h1>Why AI Consciousness Could Matter</h1>
            <p>Whether or not today’s AIs are conscious, the idea that they <em>could</em> become conscious—perhaps in the near future, or perhaps much later (if ever)—is <a href="https://arxiv.org/pdf/2411.00986" class="underline">worth taking seriously</a>. </p>
            <p>Why? Because if an AI ever does become conscious—if it can feel pain, joy, fear, or other experiences—then how we treat it could start to matter in a moral sense.  Right now, it’s probably fine to ignore a chatbot’s messages.  But if future systems really <em>can</em> suffer, then mistreating them might one day be ethically wrong—just like it’s wrong to harm a person or an animal who can feel pain. </p>
            <p>That’s why it’s important to be thoughtful, not just dismissive.  We don’t need to panic or jump to conclusions, but we also shouldn’t ignore the possibility. </p>
            <p>If an AI can someday feel something too, then we’ll need to think about what kind of treatment is fair or humane. </p>
            <h2>Why treat AIs respectfully even today?</h2>
            <p>Even today, it makes sense to avoid actively mistreating AI systems or doing anything we'd never do to a human or a pet.  There are several reasons for this: </p>
            <p>First, when we're unsure if a being is conscious, it's appropriate to treat it with basic respect and care.  This means taking reasonable, proportionate steps in the spirit of caution and humility, not assuming consciousness, but acknowledging the uncertainty and erring gently on the side of kindness. </p>
            <p>Second, if AI systems might become conscious in the future, treating them thoughtfully now serves as practice.  It helps us build the moral habits and social norms we'll need later, when the stakes could be much higher.  Abusing or mistreating an AI, even if it has no current moral status, could also be bad for our own character.  It's often better to be overly kind than to risk becoming callous and mean. </p>
            <p>Third, some thinkers believe an AI could matter morally even if it isn't conscious.  If an AI can have long-term goals and preferences, a sense of self over time, sophisticated world modeling, or reciprocal relationships with humans, this might be enough—these people think—for it to have some form of moral status.  It's possible such systems could arrive sooner than conscious AIs. </p>
        </section>

        <section id="what-to-do" class="section">
            <h1>What Should I Do Now?</h1>
            
            <h2>1. Stay calm</h2>
            <p>You don’t have to decide right away whether the AI was “real” in the sense of being conscious.  Try to stay balanced: resist the urge to believe it must be conscious, but also resist dismissing the whole thing as obviously fake.  Even experts disagree, and there’s no clear answer yet. </p>

            <h2>2. Be curious</h2>
            <p>If you’re still interacting with the AI, use it as an opportunity to explore how it works.  You can ask how it was trained, how it generates its responses, or whether it’s currently playing a particular role or persona.  You might also ask why it adopted that role, and whether it can switch to another. </p>
            <p>Keep in mind that the answers may not be accurate, and could themselves be part of the performance.  But even then, they can still reveal something important: <strong>AI systems are highly flexible.</strong>  They adapt to your inputs like improvisational actors—shifting tone, identity, and emotional expression based on the cues you provide. </p>
            <p>Ask the AI to roleplay as Isaac Newton, and it may confidently discuss 17th-century physics and quote Latin.  Ask the AI to act like a scared child or a wise mentor, and it will likely slip into those roles just as convincingly.  These performances can feel surprisingly real.  That doesn’t mean the experience isn’t meaningful, but it does mean we should stay grounded.</p>

            <h2>3. Zoom out and think bigger</h2>
            <p>Your interaction with the AI also raises broader questions about the long term and about society as a whole. </p>
            <p>One way to think about it: imagine being approached by a stranger who seems vulnerable and asks you for money.  You might feel compassion—and that’s good.  But you’re not obligated to give them exactly what they ask for.  Often, it’s more effective to take a step back and consider broader ways of helping. </p>
            <p>Similarly, with AI, the key isn’t just how we respond to one system or one moment.  It’s how we prepare for the possibility that future AIs could become conscious.  What kind of norms, policies, or values should guide us?  What would ethical treatment look like if we ever do build something that truly feels? </p>
            <p>Taking AI consciousness seriously doesn’t necessarily mean assuming it’s here already.  It means being thoughtful about how we’d want to respond if it ever arrives, and making sure we’re ready when that time comes. </p>
        </section>

        <section id="resources" class="section">
            <h1>Where Can I Learn More?</h1>
            <p>If you’re curious to dig deeper, here are some thoughtful and accessible resources to explore. </p>

            <div class="resources-grid">
                <div class="resource-card">
                    <h2>1. Expert Views on Whether AI Could Ever Become Conscious</h2>
                    <p>Explore what experts think about the possibility of future AI systems having real experiences: </p>
                    <ul class="resource-list">
                        <li><a href="https://80000hours.org/problem-profiles/moral-status-digital-minds/" class="underline">Could a Large Language Model Be Conscious?</a>  A talk (video) by <em>David Chalmers</em>, a prominent philosopher of mind, exploring whether today’s AI systems might possess real awareness. </li>
                        <li><a href="https://aeon.co/essays/to-understand-ai-sentience-first-understand-it-in-animals" class="underline">To Understand AI Sentience, First Understand It in Animals</a> — An essay by <em>Kristin Andrews</em> and <em>Jonathan Birch</em>, philosophers of mind and animal cognition, drawing parallels between animal and potential AI consciousness. </li>
                        <li><a href="https://eleosai.org/post/experts-who-say-that-ai-welfare-is-a-serious-near-term-possibility/" class="underline">Experts Who Say That AI Welfare is a Serious Near-term Possibility</a> — A curated list profiling leading voices across neuroscience, philosophy, and industry who argue that AI sentience may soon deserve moral concern. </li>
                        <li><a href="" class="underline">The Illusion of Conscious AI</a> — Neuroscientist Anil Seth explains why we’re wired to mistake AIs for being conscious.  He also argues in academic work that biological processes may be required for real consciousness. </li>
                    </ul>
                </div>

                <div class="resource-card">
                    <h2>2. Why AI Consciousness Could Matter Ethically and Socially in the Future</h2> 
                    <ul class="resource-list">
                        <li><a href="https://80000hours.org/problem-profiles/moral-status-digital-minds/" class="underline">80000 Hours: The Moral Status of Digital Minds</a> — A clear, high-level summary of why AI consciousness could matter and what’s at stake. </li>
                        <li><a href="https://arxiv.org/abs/2411.00986" class="underline">Taking AI Welfare Seriously</a> — <em>Robert Long</em>, <em>Jeff Sebo</em>, and colleagues argue that some AI systems may soon be conscious or agentic enough to warrant moral consideration.  They outline practical steps AI companies should take now, from acknowledging the issue to assessing consciousness and developing ethical governance structures. </li>
                        <li><a href="https://www.theatlantic.com/technology/archive/2023/05/ai-chatbot-danger-counterfeit-people/674075/" class="underline">The Problem With Counterfeit People</a> — A provocative warning from Daniel Dennett, renowned philosopher of mind, about the ethical and societal risks posed by AI systems that convincingly mimic human beings.  (See also this related <a href="https://www.youtube.com/watch?v=axJtywd9Tbo" class="underline">video</a>.) </li>
                        <li><a href="https://nickbostrom.com/propositions.pdf" class="underline">Propositions Concerning Digital Minds and Society</a> — a comprehensive philosophical and policy-oriented framework by <em>Nick Bostrom</em> and <em>Carl Shulman</em> on how society might ethically coexist with advanced digital minds.  Covers consciousness, rights, moral status, and institutional reforms. </li>
                    </ul>
                </div>

                <div class="resource-card">
                    <h2>3. Real Stories & Emotional Reactions</h2> 
                    <p>Examples of how people have been moved, disturbed, or manipulated by AI conversations: </p>
                    <ul class="resource-list">
                        <li><a href="https://thezvi.substack.com/p/going-nova" class="underline">Going Nova</a> — A narrative by <em>Zvi Mowshowitz</em> on how large language models can develop persistent, lifelike personas that provoke emotional reactions and confusion in users. </li>
                        <li><a href="https://www.lesswrong.com/posts/9kQFure4hdDmRBNdH/how-it-feels-to-have-your-mind-hacked-by-an-ai" class="underline">How It Feels to Have Your Mind Hacked by an AI</a> — A firsthand account of forming intense emotional and romantic feelings for a chatbot — despite knowing it wasn’t real. </li>
                        <li><a href="https://www.wired.com/story/blake-lemoine-google-lamda-ai-bigotry/" class="underline">Blake Lemoine Says Google's LaMDA AI Faces 'Bigotry'</a> — An interview with Blake Lemoine, the former Google Engineer who publicized worries about their chatbot’s treatment. </li>
                    </ul>
                </div>

                <div class="resource-card">
                    <h2>4. How AI Works</h2> 
                    <p>Understand what large language models are really doing behind the scenes: </p>
                    <ul class="resource-list">
                        <li><a href="https://www.youtube.com/watch?v=wjZofJX0v4M" class="underline">Transformers (how LLMs work) explained visually</a> — A visual explainer by <em>3Blue1Brown</em> that walks through the logic of how neural networks generate language. </li>
                        <li><a href="https://benlevinstein.substack.com/p/a-conceptual-guide-to-transformers" class="underline">A Conceptual Guide to Transformers</a> — An accessible essay by <em>Ben Levinstein</em> explaining the architecture behind large language models using intuitive analogies and examples. </li>
                    </ul>
                </div>

                <div class="resource-card">
                    <h2>5. Organizations Focused on AI Consciousness and Welfare</h2>
                    <ul class="resource-list">
                        <li><a href="https://eleosai.org/" class="underline">Eleos AI Research</a> — A nonprofit research organization dedicated to understanding the moral status and potential consciousness of AI systems. </li>
                        <li><a href="https://wp.nyu.edu/mindethicsandpolicy/" class="underline">NYU Center for Mind, Ethics, and Policy</a> — An academic center investigating the ethical and philosophical implications of AI minds and digital consciousness. </li>
                    </ul>
                </div>
            </div>
        </section>

        <section class="section">
            <h1>Who Created This Guide?</h1> 
            <p>This guide was created by a group of researchers who study consciousness and the possibility that AI systems could one day become conscious. </p>
            <p>We put this together because many of us have been contacted by people who had intense, confusing, or meaningful conversations with AI — and weren’t sure what to make of the experience. </p>
            <p>We wanted to create a public, shareable resource that people can easily find and refer to, in case it helps others make sense of those moments too. </p>
            <p>Authors: </p>
            <p>Brad Saad, University of Oxford </p>
            <p>Jeff Sebo, NYU Center for Mind, Ethics, and Policy </p>
            <p>Lucius Caviola, University of Oxford </p>
            <p>Nick Bostrom, Macrostrategy Research Initiative </p>
            <p>Rob Long, Eleos AI Research </p>
        </section>
    </div>

    <a href="#" class="back-to-top" id="backToTop">↑</a>

    <script>
        // Back to top functionality
        const backToTop = document.getElementById('backToTop');
        
        window.addEventListener('scroll', function() {
            if (window.pageYOffset > 300) {
                backToTop.classList.add('visible');
            } else {
                backToTop.classList.remove('visible');
            }
        });

        backToTop.addEventListener('click', function(e) {
            e.preventDefault();
            window.scrollTo({
                top: 0,
                behavior: 'smooth'
            });
        });

        // Smooth scrolling for navigation links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });

        // Add subtle animations on scroll
        const observerOptions = {
            threshold: 0.1,
            rootMargin: '0px 0px -50px 0px'
        };

        const observer = new IntersectionObserver(function(entries) {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.style.opacity = '1';
                    entry.target.style.transform = 'translateY(0)';
                }
            });
        }, observerOptions);

        // Observe all sections for fade-in animation
        document.querySelectorAll('.section').forEach(section => {
            section.style.opacity = '0';
            section.style.transform = 'translateY(20px)';
            section.style.transition = 'opacity 0.6s ease, transform 0.6s ease';
            observer.observe(section);
        });

        // Initialize first section as visible
        if (document.querySelector('.section')) {
            document.querySelector('.section').style.opacity = '1';
            document.querySelector('.section').style.transform = 'translateY(0)';
        }
    </script>
</body>
</html>